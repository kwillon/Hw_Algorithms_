{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXVUh-YnP2wP"
      },
      "source": [
        "# **Regression**\n",
        "1. Choose 3-4 regression models (e.g., LightGBM, XGBoost) and train them to predict “Gap” target variable on default parameters (use 10-fold cross-validation)\n",
        "Note: every model has use example on scikit-learn.org\n",
        "\n",
        "2. Visualize the results via R2 plot (predicted vs. real), compare the models’ performance and training speed\n",
        "\n",
        "3. Optimize the best-performing model using hyperparameters tuning (grid search)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i3KQGN1NOYq2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2QBZwKDEowyg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kru3xo4FT2zq"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9SeCNqB-vCCi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOG30kS-AFau"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S9pQp6ECQf_S"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/df_CPs.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fum54mM_jjh_"
      },
      "source": [
        "# Раздел борьбы со smiles, который я для чего-то берегла, можно не смотреть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0gI0WUCuii6"
      },
      "source": [
        "Скорее всего их надо было закодировать с самого начала или выбросить сейчас, но мне казалось, что со smiles понятней для выбора молекул, обычно я не работаю со smiles и могу ошибаться"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYwlv7ZugYr9",
        "outputId": "10dcced8-5a14-4de4-d3ca-b3c2e80e43fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19981 entries, 0 to 19980\n",
            "Data columns (total 62 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Unnamed: 0  19981 non-null  int64  \n",
            " 1   PC1         19981 non-null  float64\n",
            " 2   PC2         19981 non-null  float64\n",
            " 3   PC3         19981 non-null  float64\n",
            " 4   PC4         19981 non-null  float64\n",
            " 5   PC5         19981 non-null  float64\n",
            " 6   PC6         19981 non-null  float64\n",
            " 7   PC7         19981 non-null  float64\n",
            " 8   PC8         19981 non-null  float64\n",
            " 9   PC9         19981 non-null  float64\n",
            " 10  PC10        19981 non-null  float64\n",
            " 11  PC11        19981 non-null  float64\n",
            " 12  PC12        19981 non-null  float64\n",
            " 13  PC13        19981 non-null  float64\n",
            " 14  PC14        19981 non-null  float64\n",
            " 15  PC15        19981 non-null  float64\n",
            " 16  PC16        19981 non-null  float64\n",
            " 17  PC17        19981 non-null  float64\n",
            " 18  PC18        19981 non-null  float64\n",
            " 19  PC19        19981 non-null  float64\n",
            " 20  PC20        19981 non-null  float64\n",
            " 21  PC21        19981 non-null  float64\n",
            " 22  PC22        19981 non-null  float64\n",
            " 23  PC23        19981 non-null  float64\n",
            " 24  PC24        19981 non-null  float64\n",
            " 25  PC25        19981 non-null  float64\n",
            " 26  PC26        19981 non-null  float64\n",
            " 27  PC27        19981 non-null  float64\n",
            " 28  PC28        19981 non-null  float64\n",
            " 29  PC29        19981 non-null  float64\n",
            " 30  PC30        19981 non-null  float64\n",
            " 31  PC31        19981 non-null  float64\n",
            " 32  PC32        19981 non-null  float64\n",
            " 33  PC33        19981 non-null  float64\n",
            " 34  PC34        19981 non-null  float64\n",
            " 35  PC35        19981 non-null  float64\n",
            " 36  PC36        19981 non-null  float64\n",
            " 37  PC37        19981 non-null  float64\n",
            " 38  PC38        19981 non-null  float64\n",
            " 39  PC39        19981 non-null  float64\n",
            " 40  PC40        19981 non-null  float64\n",
            " 41  PC41        19981 non-null  float64\n",
            " 42  PC42        19981 non-null  float64\n",
            " 43  PC43        19981 non-null  float64\n",
            " 44  PC44        19981 non-null  float64\n",
            " 45  PC45        19981 non-null  float64\n",
            " 46  PC46        19981 non-null  float64\n",
            " 47  PC47        19981 non-null  float64\n",
            " 48  PC48        19981 non-null  float64\n",
            " 49  PC49        19981 non-null  float64\n",
            " 50  PC50        19981 non-null  float64\n",
            " 51  PC51        19981 non-null  float64\n",
            " 52  PC52        19981 non-null  float64\n",
            " 53  PC53        19981 non-null  float64\n",
            " 54  PC54        19981 non-null  float64\n",
            " 55  PC55        19981 non-null  float64\n",
            " 56  PC56        19981 non-null  float64\n",
            " 57  PC57        19981 non-null  float64\n",
            " 58  PC58        19981 non-null  float64\n",
            " 59  PC59        19981 non-null  float64\n",
            " 60  smiles      19964 non-null  object \n",
            " 61  gap         19964 non-null  float64\n",
            "dtypes: float64(60), int64(1), object(1)\n",
            "memory usage: 9.5+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jIwTBu40eZRq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "smiles_encoded = label_encoder.fit_transform(df['smiles'])\n",
        "encoded_df = pd.DataFrame(smiles_encoded, columns=['encoded_smiles'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G8ZlHlRe6eg",
        "outputId": "6338b878-ada0-4256-89d5-4ebd10a1402e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19981 entries, 0 to 19980\n",
            "Data columns (total 63 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Unnamed: 0      19981 non-null  int64  \n",
            " 1   PC1             19981 non-null  float64\n",
            " 2   PC2             19981 non-null  float64\n",
            " 3   PC3             19981 non-null  float64\n",
            " 4   PC4             19981 non-null  float64\n",
            " 5   PC5             19981 non-null  float64\n",
            " 6   PC6             19981 non-null  float64\n",
            " 7   PC7             19981 non-null  float64\n",
            " 8   PC8             19981 non-null  float64\n",
            " 9   PC9             19981 non-null  float64\n",
            " 10  PC10            19981 non-null  float64\n",
            " 11  PC11            19981 non-null  float64\n",
            " 12  PC12            19981 non-null  float64\n",
            " 13  PC13            19981 non-null  float64\n",
            " 14  PC14            19981 non-null  float64\n",
            " 15  PC15            19981 non-null  float64\n",
            " 16  PC16            19981 non-null  float64\n",
            " 17  PC17            19981 non-null  float64\n",
            " 18  PC18            19981 non-null  float64\n",
            " 19  PC19            19981 non-null  float64\n",
            " 20  PC20            19981 non-null  float64\n",
            " 21  PC21            19981 non-null  float64\n",
            " 22  PC22            19981 non-null  float64\n",
            " 23  PC23            19981 non-null  float64\n",
            " 24  PC24            19981 non-null  float64\n",
            " 25  PC25            19981 non-null  float64\n",
            " 26  PC26            19981 non-null  float64\n",
            " 27  PC27            19981 non-null  float64\n",
            " 28  PC28            19981 non-null  float64\n",
            " 29  PC29            19981 non-null  float64\n",
            " 30  PC30            19981 non-null  float64\n",
            " 31  PC31            19981 non-null  float64\n",
            " 32  PC32            19981 non-null  float64\n",
            " 33  PC33            19981 non-null  float64\n",
            " 34  PC34            19981 non-null  float64\n",
            " 35  PC35            19981 non-null  float64\n",
            " 36  PC36            19981 non-null  float64\n",
            " 37  PC37            19981 non-null  float64\n",
            " 38  PC38            19981 non-null  float64\n",
            " 39  PC39            19981 non-null  float64\n",
            " 40  PC40            19981 non-null  float64\n",
            " 41  PC41            19981 non-null  float64\n",
            " 42  PC42            19981 non-null  float64\n",
            " 43  PC43            19981 non-null  float64\n",
            " 44  PC44            19981 non-null  float64\n",
            " 45  PC45            19981 non-null  float64\n",
            " 46  PC46            19981 non-null  float64\n",
            " 47  PC47            19981 non-null  float64\n",
            " 48  PC48            19981 non-null  float64\n",
            " 49  PC49            19981 non-null  float64\n",
            " 50  PC50            19981 non-null  float64\n",
            " 51  PC51            19981 non-null  float64\n",
            " 52  PC52            19981 non-null  float64\n",
            " 53  PC53            19981 non-null  float64\n",
            " 54  PC54            19981 non-null  float64\n",
            " 55  PC55            19981 non-null  float64\n",
            " 56  PC56            19981 non-null  float64\n",
            " 57  PC57            19981 non-null  float64\n",
            " 58  PC58            19981 non-null  float64\n",
            " 59  PC59            19981 non-null  float64\n",
            " 60  smiles          19964 non-null  object \n",
            " 61  gap             19964 non-null  float64\n",
            " 62  encoded_smiles  19981 non-null  int64  \n",
            "dtypes: float64(60), int64(2), object(1)\n",
            "memory usage: 9.6+ MB\n"
          ]
        }
      ],
      "source": [
        "df_encoded = pd.concat([df, encoded_df], axis=1)\n",
        "df = df_encoded\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HdKthmAihePP"
      },
      "outputs": [],
      "source": [
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "#Нормализуйте столбец \n",
        "encoded_smiles_normalized = minmax_scaler.fit_transform(encoded_df[['encoded_smiles']])\n",
        "\n",
        "#DataFrame с нормализованными значениями SMILES\n",
        "normalized_encoded_df = pd.DataFrame(encoded_smiles_normalized, columns=['normalized_encoded_smiles'])\n",
        "\n",
        "#Добавить нормализованные значения обратно в исходный DataFrame\n",
        "df_encoded_normalized = pd.concat([df_encoded, normalized_encoded_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXhbiZapg8Z7",
        "outputId": "c8a27d61-be8a-4f69-b1bc-aa0cec119661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        0.826938\n",
              "1        0.638650\n",
              "2        0.904829\n",
              "3        0.720998\n",
              "4        0.132488\n",
              "           ...   \n",
              "19976    0.997646\n",
              "19977    0.111501\n",
              "19978    0.675917\n",
              "19979    0.968443\n",
              "19980    0.976057\n",
              "Name: normalized_encoded_smiles, Length: 19981, dtype: float64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df =df_encoded_normalized\n",
        "df = df.drop(columns=['smiles', 'encoded_smiles'])\n",
        "df['normalized_encoded_smiles']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_juhTUzRLsl",
        "outputId": "caec9620-8474-463a-f0e2-d6fb59a1096e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19981 entries, 0 to 19980\n",
            "Data columns (total 62 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   Unnamed: 0                 19981 non-null  int64  \n",
            " 1   PC1                        19981 non-null  float64\n",
            " 2   PC2                        19981 non-null  float64\n",
            " 3   PC3                        19981 non-null  float64\n",
            " 4   PC4                        19981 non-null  float64\n",
            " 5   PC5                        19981 non-null  float64\n",
            " 6   PC6                        19981 non-null  float64\n",
            " 7   PC7                        19981 non-null  float64\n",
            " 8   PC8                        19981 non-null  float64\n",
            " 9   PC9                        19981 non-null  float64\n",
            " 10  PC10                       19981 non-null  float64\n",
            " 11  PC11                       19981 non-null  float64\n",
            " 12  PC12                       19981 non-null  float64\n",
            " 13  PC13                       19981 non-null  float64\n",
            " 14  PC14                       19981 non-null  float64\n",
            " 15  PC15                       19981 non-null  float64\n",
            " 16  PC16                       19981 non-null  float64\n",
            " 17  PC17                       19981 non-null  float64\n",
            " 18  PC18                       19981 non-null  float64\n",
            " 19  PC19                       19981 non-null  float64\n",
            " 20  PC20                       19981 non-null  float64\n",
            " 21  PC21                       19981 non-null  float64\n",
            " 22  PC22                       19981 non-null  float64\n",
            " 23  PC23                       19981 non-null  float64\n",
            " 24  PC24                       19981 non-null  float64\n",
            " 25  PC25                       19981 non-null  float64\n",
            " 26  PC26                       19981 non-null  float64\n",
            " 27  PC27                       19981 non-null  float64\n",
            " 28  PC28                       19981 non-null  float64\n",
            " 29  PC29                       19981 non-null  float64\n",
            " 30  PC30                       19981 non-null  float64\n",
            " 31  PC31                       19981 non-null  float64\n",
            " 32  PC32                       19981 non-null  float64\n",
            " 33  PC33                       19981 non-null  float64\n",
            " 34  PC34                       19981 non-null  float64\n",
            " 35  PC35                       19981 non-null  float64\n",
            " 36  PC36                       19981 non-null  float64\n",
            " 37  PC37                       19981 non-null  float64\n",
            " 38  PC38                       19981 non-null  float64\n",
            " 39  PC39                       19981 non-null  float64\n",
            " 40  PC40                       19981 non-null  float64\n",
            " 41  PC41                       19981 non-null  float64\n",
            " 42  PC42                       19981 non-null  float64\n",
            " 43  PC43                       19981 non-null  float64\n",
            " 44  PC44                       19981 non-null  float64\n",
            " 45  PC45                       19981 non-null  float64\n",
            " 46  PC46                       19981 non-null  float64\n",
            " 47  PC47                       19981 non-null  float64\n",
            " 48  PC48                       19981 non-null  float64\n",
            " 49  PC49                       19981 non-null  float64\n",
            " 50  PC50                       19981 non-null  float64\n",
            " 51  PC51                       19981 non-null  float64\n",
            " 52  PC52                       19981 non-null  float64\n",
            " 53  PC53                       19981 non-null  float64\n",
            " 54  PC54                       19981 non-null  float64\n",
            " 55  PC55                       19981 non-null  float64\n",
            " 56  PC56                       19981 non-null  float64\n",
            " 57  PC57                       19981 non-null  float64\n",
            " 58  PC58                       19981 non-null  float64\n",
            " 59  PC59                       19981 non-null  float64\n",
            " 60  gap                        19964 non-null  float64\n",
            " 61  normalized_encoded_smiles  19981 non-null  float64\n",
            "dtypes: float64(61), int64(1)\n",
            "memory usage: 9.5 MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T5XpTI3lkFUG"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "lypXERYHtNMv",
        "outputId": "807dc71a-1613-43d9-b926-edfe5e2da8cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-d80329a8-2bba-4419-bb49-635a844dce37\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PC1</th>\n",
              "      <th>PC2</th>\n",
              "      <th>PC3</th>\n",
              "      <th>PC4</th>\n",
              "      <th>PC5</th>\n",
              "      <th>PC6</th>\n",
              "      <th>PC7</th>\n",
              "      <th>PC8</th>\n",
              "      <th>PC9</th>\n",
              "      <th>PC10</th>\n",
              "      <th>...</th>\n",
              "      <th>PC52</th>\n",
              "      <th>PC53</th>\n",
              "      <th>PC54</th>\n",
              "      <th>PC55</th>\n",
              "      <th>PC56</th>\n",
              "      <th>PC57</th>\n",
              "      <th>PC58</th>\n",
              "      <th>PC59</th>\n",
              "      <th>gap</th>\n",
              "      <th>normalized_encoded_smiles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.339214</td>\n",
              "      <td>0.442302</td>\n",
              "      <td>0.159721</td>\n",
              "      <td>-0.461903</td>\n",
              "      <td>-0.236782</td>\n",
              "      <td>0.702281</td>\n",
              "      <td>0.176595</td>\n",
              "      <td>0.378853</td>\n",
              "      <td>-0.370039</td>\n",
              "      <td>0.382089</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.080061</td>\n",
              "      <td>0.067852</td>\n",
              "      <td>-0.039111</td>\n",
              "      <td>0.154440</td>\n",
              "      <td>0.079698</td>\n",
              "      <td>-0.043792</td>\n",
              "      <td>-0.117808</td>\n",
              "      <td>-0.022823</td>\n",
              "      <td>0.2258</td>\n",
              "      <td>0.826938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.643303</td>\n",
              "      <td>0.098820</td>\n",
              "      <td>0.165701</td>\n",
              "      <td>-0.437282</td>\n",
              "      <td>0.104974</td>\n",
              "      <td>0.426370</td>\n",
              "      <td>0.457961</td>\n",
              "      <td>-0.110241</td>\n",
              "      <td>-0.371135</td>\n",
              "      <td>-0.127971</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044334</td>\n",
              "      <td>0.051943</td>\n",
              "      <td>-0.048330</td>\n",
              "      <td>0.059093</td>\n",
              "      <td>0.093159</td>\n",
              "      <td>0.036363</td>\n",
              "      <td>0.025190</td>\n",
              "      <td>0.091761</td>\n",
              "      <td>0.1826</td>\n",
              "      <td>0.638650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.126477</td>\n",
              "      <td>-0.011759</td>\n",
              "      <td>0.013490</td>\n",
              "      <td>-0.156802</td>\n",
              "      <td>-0.393077</td>\n",
              "      <td>0.036280</td>\n",
              "      <td>0.045197</td>\n",
              "      <td>0.235364</td>\n",
              "      <td>0.210192</td>\n",
              "      <td>-0.173686</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.024181</td>\n",
              "      <td>-0.006624</td>\n",
              "      <td>0.005069</td>\n",
              "      <td>-0.043225</td>\n",
              "      <td>-0.001808</td>\n",
              "      <td>0.049079</td>\n",
              "      <td>-0.079340</td>\n",
              "      <td>0.044904</td>\n",
              "      <td>0.2209</td>\n",
              "      <td>0.904829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.078209</td>\n",
              "      <td>-0.031696</td>\n",
              "      <td>0.678240</td>\n",
              "      <td>0.245904</td>\n",
              "      <td>0.196744</td>\n",
              "      <td>0.980741</td>\n",
              "      <td>-0.133933</td>\n",
              "      <td>-0.033095</td>\n",
              "      <td>-0.289892</td>\n",
              "      <td>-0.252373</td>\n",
              "      <td>...</td>\n",
              "      <td>0.106457</td>\n",
              "      <td>0.105640</td>\n",
              "      <td>-0.011042</td>\n",
              "      <td>-0.006389</td>\n",
              "      <td>-0.051015</td>\n",
              "      <td>-0.096002</td>\n",
              "      <td>0.141883</td>\n",
              "      <td>0.029436</td>\n",
              "      <td>0.2368</td>\n",
              "      <td>0.720998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.135424</td>\n",
              "      <td>-0.594030</td>\n",
              "      <td>-0.330410</td>\n",
              "      <td>-0.079832</td>\n",
              "      <td>0.133053</td>\n",
              "      <td>-0.016324</td>\n",
              "      <td>0.335906</td>\n",
              "      <td>-0.153286</td>\n",
              "      <td>0.355254</td>\n",
              "      <td>-0.012943</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.088894</td>\n",
              "      <td>-0.042282</td>\n",
              "      <td>0.083902</td>\n",
              "      <td>-0.052619</td>\n",
              "      <td>0.028385</td>\n",
              "      <td>-0.080537</td>\n",
              "      <td>-0.009442</td>\n",
              "      <td>-0.135750</td>\n",
              "      <td>0.2507</td>\n",
              "      <td>0.132488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19976</th>\n",
              "      <td>-0.033615</td>\n",
              "      <td>0.437328</td>\n",
              "      <td>0.373794</td>\n",
              "      <td>0.133691</td>\n",
              "      <td>0.419226</td>\n",
              "      <td>-0.151528</td>\n",
              "      <td>-0.095548</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>-0.008845</td>\n",
              "      <td>-0.100168</td>\n",
              "      <td>...</td>\n",
              "      <td>0.071868</td>\n",
              "      <td>-0.029205</td>\n",
              "      <td>0.033890</td>\n",
              "      <td>-0.022481</td>\n",
              "      <td>-0.040726</td>\n",
              "      <td>-0.022581</td>\n",
              "      <td>0.031258</td>\n",
              "      <td>0.030001</td>\n",
              "      <td>0.2503</td>\n",
              "      <td>0.997646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19977</th>\n",
              "      <td>-0.049046</td>\n",
              "      <td>-0.590533</td>\n",
              "      <td>-0.218991</td>\n",
              "      <td>-0.091756</td>\n",
              "      <td>0.087148</td>\n",
              "      <td>-0.033772</td>\n",
              "      <td>-0.129681</td>\n",
              "      <td>-0.096177</td>\n",
              "      <td>-0.394211</td>\n",
              "      <td>-0.224154</td>\n",
              "      <td>...</td>\n",
              "      <td>0.110349</td>\n",
              "      <td>0.148339</td>\n",
              "      <td>-0.173841</td>\n",
              "      <td>-0.032819</td>\n",
              "      <td>0.140679</td>\n",
              "      <td>-0.028995</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>0.006364</td>\n",
              "      <td>0.2493</td>\n",
              "      <td>0.111501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19978</th>\n",
              "      <td>0.014199</td>\n",
              "      <td>-0.728073</td>\n",
              "      <td>-0.453850</td>\n",
              "      <td>-0.100943</td>\n",
              "      <td>-0.091309</td>\n",
              "      <td>0.015275</td>\n",
              "      <td>-0.036137</td>\n",
              "      <td>-0.476937</td>\n",
              "      <td>-0.089477</td>\n",
              "      <td>-0.592685</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026064</td>\n",
              "      <td>-0.007584</td>\n",
              "      <td>0.105254</td>\n",
              "      <td>0.136299</td>\n",
              "      <td>0.096911</td>\n",
              "      <td>0.062190</td>\n",
              "      <td>-0.039858</td>\n",
              "      <td>-0.037964</td>\n",
              "      <td>0.2342</td>\n",
              "      <td>0.675917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19979</th>\n",
              "      <td>0.889089</td>\n",
              "      <td>0.285766</td>\n",
              "      <td>-0.114927</td>\n",
              "      <td>-0.063618</td>\n",
              "      <td>0.015302</td>\n",
              "      <td>-0.109214</td>\n",
              "      <td>0.455168</td>\n",
              "      <td>0.358017</td>\n",
              "      <td>-0.187704</td>\n",
              "      <td>0.012368</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.200711</td>\n",
              "      <td>-0.028286</td>\n",
              "      <td>0.086818</td>\n",
              "      <td>0.095097</td>\n",
              "      <td>-0.009952</td>\n",
              "      <td>-0.041632</td>\n",
              "      <td>-0.021918</td>\n",
              "      <td>-0.087378</td>\n",
              "      <td>0.2179</td>\n",
              "      <td>0.968443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19980</th>\n",
              "      <td>-0.247872</td>\n",
              "      <td>-0.023133</td>\n",
              "      <td>-0.245990</td>\n",
              "      <td>-0.126018</td>\n",
              "      <td>-0.060975</td>\n",
              "      <td>-0.025004</td>\n",
              "      <td>-0.084973</td>\n",
              "      <td>-0.244461</td>\n",
              "      <td>-0.197616</td>\n",
              "      <td>0.018306</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012710</td>\n",
              "      <td>-0.083240</td>\n",
              "      <td>0.038239</td>\n",
              "      <td>-0.048588</td>\n",
              "      <td>-0.029850</td>\n",
              "      <td>0.004309</td>\n",
              "      <td>0.061842</td>\n",
              "      <td>0.038601</td>\n",
              "      <td>0.2424</td>\n",
              "      <td>0.976057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19964 rows × 61 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d80329a8-2bba-4419-bb49-635a844dce37')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d80329a8-2bba-4419-bb49-635a844dce37 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d80329a8-2bba-4419-bb49-635a844dce37');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-326c599e-a5bd-4104-90a6-5bb16e328e11\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-326c599e-a5bd-4104-90a6-5bb16e328e11')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-326c599e-a5bd-4104-90a6-5bb16e328e11 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
              "0     -0.339214  0.442302  0.159721 -0.461903 -0.236782  0.702281  0.176595   \n",
              "1     -0.643303  0.098820  0.165701 -0.437282  0.104974  0.426370  0.457961   \n",
              "2      0.126477 -0.011759  0.013490 -0.156802 -0.393077  0.036280  0.045197   \n",
              "3     -0.078209 -0.031696  0.678240  0.245904  0.196744  0.980741 -0.133933   \n",
              "4     -0.135424 -0.594030 -0.330410 -0.079832  0.133053 -0.016324  0.335906   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "19976 -0.033615  0.437328  0.373794  0.133691  0.419226 -0.151528 -0.095548   \n",
              "19977 -0.049046 -0.590533 -0.218991 -0.091756  0.087148 -0.033772 -0.129681   \n",
              "19978  0.014199 -0.728073 -0.453850 -0.100943 -0.091309  0.015275 -0.036137   \n",
              "19979  0.889089  0.285766 -0.114927 -0.063618  0.015302 -0.109214  0.455168   \n",
              "19980 -0.247872 -0.023133 -0.245990 -0.126018 -0.060975 -0.025004 -0.084973   \n",
              "\n",
              "            PC8       PC9      PC10  ...      PC52      PC53      PC54  \\\n",
              "0      0.378853 -0.370039  0.382089  ... -0.080061  0.067852 -0.039111   \n",
              "1     -0.110241 -0.371135 -0.127971  ... -0.044334  0.051943 -0.048330   \n",
              "2      0.235364  0.210192 -0.173686  ... -0.024181 -0.006624  0.005069   \n",
              "3     -0.033095 -0.289892 -0.252373  ...  0.106457  0.105640 -0.011042   \n",
              "4     -0.153286  0.355254 -0.012943  ... -0.088894 -0.042282  0.083902   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "19976  0.001703 -0.008845 -0.100168  ...  0.071868 -0.029205  0.033890   \n",
              "19977 -0.096177 -0.394211 -0.224154  ...  0.110349  0.148339 -0.173841   \n",
              "19978 -0.476937 -0.089477 -0.592685  ... -0.026064 -0.007584  0.105254   \n",
              "19979  0.358017 -0.187704  0.012368  ... -0.200711 -0.028286  0.086818   \n",
              "19980 -0.244461 -0.197616  0.018306  ...  0.012710 -0.083240  0.038239   \n",
              "\n",
              "           PC55      PC56      PC57      PC58      PC59     gap  \\\n",
              "0      0.154440  0.079698 -0.043792 -0.117808 -0.022823  0.2258   \n",
              "1      0.059093  0.093159  0.036363  0.025190  0.091761  0.1826   \n",
              "2     -0.043225 -0.001808  0.049079 -0.079340  0.044904  0.2209   \n",
              "3     -0.006389 -0.051015 -0.096002  0.141883  0.029436  0.2368   \n",
              "4     -0.052619  0.028385 -0.080537 -0.009442 -0.135750  0.2507   \n",
              "...         ...       ...       ...       ...       ...     ...   \n",
              "19976 -0.022481 -0.040726 -0.022581  0.031258  0.030001  0.2503   \n",
              "19977 -0.032819  0.140679 -0.028995  0.001284  0.006364  0.2493   \n",
              "19978  0.136299  0.096911  0.062190 -0.039858 -0.037964  0.2342   \n",
              "19979  0.095097 -0.009952 -0.041632 -0.021918 -0.087378  0.2179   \n",
              "19980 -0.048588 -0.029850  0.004309  0.061842  0.038601  0.2424   \n",
              "\n",
              "       normalized_encoded_smiles  \n",
              "0                       0.826938  \n",
              "1                       0.638650  \n",
              "2                       0.904829  \n",
              "3                       0.720998  \n",
              "4                       0.132488  \n",
              "...                          ...  \n",
              "19976                   0.997646  \n",
              "19977                   0.111501  \n",
              "19978                   0.675917  \n",
              "19979                   0.968443  \n",
              "19980                   0.976057  \n",
              "\n",
              "[19964 rows x 61 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Строка, чтобы переименовать столбцы так, чтобы они содержали только буквы, цифры и символ подчеркивания\n",
        "df.columns = [\"\".join(c if c.isalnum() or c == \"_\" else \"_\" for c in str(col)) for col in df.columns]\n",
        "df = df.drop('Unnamed__0', axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVm8lipSjwCP"
      },
      "source": [
        "# Построим модели LightGBM, XGBoost и RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5OjC3Djg2V6X"
      },
      "outputs": [],
      "source": [
        "#Будем сранивать скорость обучения\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLmD_Elwj-fy",
        "outputId": "91250a7a-1dd1-4da2-85ad-16bb43c637d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n"
          ]
        }
      ],
      "source": [
        "X = df.drop('gap', axis=1)  # 'gap' - это целевая переменная\n",
        "y = df['gap']\n",
        "\n",
        "#модели\n",
        "dt_regressor = DecisionTreeRegressor()\n",
        "rf_regressor = RandomForestRegressor()\n",
        "xgb_regressor = XGBRegressor()\n",
        "lgbm_regressor = LGBMRegressor(force_col_wise=True)\n",
        "\n",
        "start_time = time.time()\n",
        "rf_scores = cross_val_score(rf_regressor, X, y, cv=10, scoring='r2')\n",
        "rf_training_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "xgb_scores = cross_val_score(xgb_regressor, X, y, cv=10, scoring='r2')\n",
        "xgb_training_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "lgbm_scores = cross_val_score(lgbm_regressor, X, y, cv=10, scoring='r2')\n",
        "lgbm_training_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "dt_scores = cross_val_score(dt_regressor, X, y, cv=10, scoring='r2')\n",
        "dt_training_time = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khXAaso23y2h",
        "outputId": "2f9dc34a-e999-4a88-ef83-3eb56012959c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Время обучения каждой модели: RandomForest=1435.9431130886078, XGBoost=36.729125022888184, LightGBM=19.159460067749023, DecisionTree=21.31606364250183\n"
          ]
        }
      ],
      "source": [
        "print(f'Время обучения каждой модели: RandomForest={rf_training_time}, XGBoost={xgb_training_time}, LightGBM={lgbm_training_time}, DecisionTree={dt_training_time}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK3vS1fY5oBJ"
      },
      "source": [
        "Как можно заметить модель рандомного леса работала очень долго, и модель LGB справилась быстрее всего."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4uCKoJm3KEX",
        "outputId": "81fbc8ee-adc7-4f60-f759-e640eeaaf590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest R^2: 0.31006026881356297\n",
            "XGBoost R^2: 0.1820377086154617\n",
            "LightGBM R^2: 0.25114686230953553\n",
            "Decision Tree R^2: -0.3046750313441247\n"
          ]
        }
      ],
      "source": [
        "print(\"Random Forest R^2:\", rf_scores.mean()) #Описывает лучше остальных, но жутко долго учится\n",
        "print(\"XGBoost R^2:\", xgb_scores.mean())\n",
        "print(\"LightGBM R^2:\", lgbm_scores.mean())\n",
        "print(\"Decision Tree R^2:\", dt_scores.mean()) #Возможно эта модель не подходит для таких данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lClbPhpFiAOz"
      },
      "outputs": [],
      "source": [
        "#Сравним r^2 для каждой модели\n",
        "r2_scores = [dt_scores.mean(), rf_scores.mean(), lgbm_scores.mean(), xgb_scores.mean()]\n",
        "models = ['DT', 'RF', 'GBM', 'XGB']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "8Y44hEhYoGYV",
        "outputId": "24b026dd-e63d-4b13-a605-b88a8a9dcc8b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.bar(models, r2_scores, color=['blue', 'green', 'red', 'magenta'])\n",
        "plt.xlabel('Все модели')\n",
        "plt.ylabel('R^2')\n",
        "plt.title('Сравнение R^2 для 4 моделей')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ3rF2Ro0QGB"
      },
      "source": [
        "Как видно на графике выше r^2 лучше всего у RandomForest, однака всё ещё очень далека от 1, то есть до предсказания данных наиболее полно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3J8m3ykurwa"
      },
      "source": [
        "Подбор оптимальных гиперпараметров для LGBMRegressor, так как у неё оптимальное соотношение время и r^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEdmule_uuUB",
        "outputId": "ac1aded5-4cac-4040-c3b1-a86d5e8b7b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251694\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251703\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251616\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17967, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251934\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251834\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251767\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251611\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251676\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251736\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 17968, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251938\n",
            "[LightGBM] [Info] Total Bins 15300\n",
            "[LightGBM] [Info] Number of data points in the train set: 19964, number of used features: 60\n",
            "[LightGBM] [Info] Start training from score 0.251751\n",
            "Наилучшие параметры: {'max_depth': 20, 'n_estimators': 20, 'num_leaves': 50} для LGBMRegressor\n",
            "Наилучшее значение R^2: 0.2621641554928166\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [20, 50, 100],     #Количество деревьев\n",
        "    'max_depth': [10, 20],             #Максимальная глубина\n",
        "    'num_leaves': [31, 50]             #Количество листьев\n",
        "}\n",
        "\n",
        "lgbm_regressor = LGBMRegressor(force_col_wise=True)\n",
        "grid_search_lgbm = GridSearchCV(lgbm_regressor, param_grid, cv=10)\n",
        "grid_search_lgbm.fit(X, y)\n",
        "\n",
        "print(f'Наилучшие параметры: {grid_search_lgbm.best_params_} для LGBMRegressor')\n",
        "print(f'Наилучшее значение R^2: {grid_search_lgbm.best_score_}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54BiWxx38yy0"
      },
      "source": [
        "Было R^2=0.25114686230953553, стало r^2=0.2621641554928166, незначительно улучшилось, но всё же надо выбрать другую модель для предсказания"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fum54mM_jjh_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
